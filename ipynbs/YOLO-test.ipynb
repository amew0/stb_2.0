{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch #installed -no need to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ultralytics/ultralytics.git # cloned -no need to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ultralytics/requirements.txt #requirements fulfilled -no need to run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics #done -no need to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLI option\n",
    "#!yolo task=detect mode=predict model=yolov8x.pt source=\"\" show=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "detection_model = YOLO(\"yolov8x.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locateObj(res,image):\n",
    "    boxes=res[0].boxes\n",
    "    box=boxes[0]\n",
    "    x1=int(box.xyxy[0][0])\n",
    "    y1=int(box.xyxy[0][1])\n",
    "    x2=int(box.xyxy[0][2])\n",
    "    y2=int(box.xyxy[0][3])\n",
    "    img = cv2.imread(image)\n",
    "    img_np = np.asarray(img)\n",
    "    bounded_img= img_np[y1:y2,x1:x2]\n",
    "    return bounded_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ai-05/Desktop/sdp/Custom-Dataset/Plastic/frame_000389.PNG: 384x640 1 bottle, 26.7ms\n",
      "Speed: 0.4ms preprocess, 26.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "image=\"/home/ai-05/Desktop/sdp/Custom-Dataset/Plastic/frame_000389.PNG\"\n",
    "results = detection_model.predict(source=image,show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(results[0].boxes)=0 means no detection and > 1 means multiple objects detected\n",
    "objs = len(results[0].boxes)\n",
    "if(objs ==0 or objs>1):\n",
    "    pass # to be replace by call to the classifier sending the entire captured image\n",
    "elif(objs==1):\n",
    "    bounded_img=locateObj(results,image)\n",
    "    cv2.imshow(\"Bounded Frame\",bounded_img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    #pass bounded frame to the classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/ai-05/miniconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "2023-04-03 15:43:00.638626: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Ultralytics YOLOv8.0.59 ðŸš€ Python-3.10.9 torch-2.0.0+cu117 CPU\n",
      "YOLOv8x summary (fused): 268 layers, 68200608 parameters, 0 gradients, 257.8 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from yolov8x.pt with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (130.5 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting export with tensorflow 2.12.0...\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.13.1...\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.0+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m simplifying with onnxsim 0.4.19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 18.8s, saved as yolov8x.onnx (260.4 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m running 'onnx2tf -i yolov8x.onnx -o yolov8x_saved_model -nuo --non_verbose'\n",
      "2023-04-03 15:43:35.911462: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m export success âœ… 33.6s, saved as yolov8x_saved_model (651.1 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorFlow Lite:\u001b[0m starting export with tensorflow 2.12.0...\n",
      "\u001b[34m\u001b[1mTensorFlow Lite:\u001b[0m export success âœ… 0.0s, saved as yolov8x_saved_model/yolov8x_float32.tflite (260.3 MB)\n",
      "\n",
      "Export complete (34.9s)\n",
      "Results saved to \u001b[1m/home/ai-05/Desktop/sdp\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolov8x_saved_model/yolov8x_float32.tflite imgsz=640 \n",
      "Validate:        yolo val task=detect model=yolov8x_saved_model/yolov8x_float32.tflite imgsz=640 data=coco.yaml \n",
      "Visualize:       https://netron.app\n"
     ]
    }
   ],
   "source": [
    "!yolo export model=yolov8x.pt format='tflite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "onnx_model = onnx.load(\"yolov8x.onnx\")\n",
    "\n",
    "from onnx_tf.backend import prepare\n",
    "tf_rep = prepare(onnx_model)\n",
    "\n",
    "tf_rep.export_graph(\"/home/ai-05/Desktop/sdp/\")\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.saved_model.load(tf_model_path)\n",
    "model.trainable = False\n",
    "\n",
    "input_tensor = tf.random.uniform([batch_size, channels, height, width])\n",
    "out = model(**{'input': input_tensor})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
